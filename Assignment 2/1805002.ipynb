{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                 'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "                 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "dataset1 = pd.read_csv('datasets/1/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "dataset2_train = pd.read_csv('datasets/2/adult.data', header=None, names=missing_columns, na_values=' ?')\n",
    "dataset2_test = pd.read_csv('datasets/2/adult.test', header=None, names=missing_columns, na_values=' ?')\n",
    "dataset2_train['income'] = dataset2_train['income'].str.replace('.', '')\n",
    "dataset2_test['income'] = dataset2_test['income'].str.replace('.', '')\n",
    "dataset2 = pd.concat([dataset2_train, dataset2_test])\n",
    "\n",
    "dataset3 = pd.read_csv('datasets/3/creditcard.csv')\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "#dataset = dataset1\n",
    "#dataset = dataset2\n",
    "#dataset = dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = 3\n",
    "\n",
    "\n",
    "if dataset_type == 1:\n",
    "    dataset = dataset1\n",
    "elif dataset_type == 2:\n",
    "    dataset = dataset2\n",
    "elif dataset_type == 3:\n",
    "    dataset = dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 48843 entries, 0 to 16281\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   age             48843 non-null  object \n",
      " 1   workclass       46043 non-null  object \n",
      " 2   fnlwgt          48842 non-null  float64\n",
      " 3   education       48842 non-null  object \n",
      " 4   education-num   48842 non-null  float64\n",
      " 5   marital-status  48842 non-null  object \n",
      " 6   occupation      46033 non-null  object \n",
      " 7   relationship    48842 non-null  object \n",
      " 8   race            48842 non-null  object \n",
      " 9   sex             48842 non-null  object \n",
      " 10  capital-gain    48842 non-null  float64\n",
      " 11  capital-loss    48842 non-null  float64\n",
      " 12  hours-per-week  48842 non-null  float64\n",
      " 13  native-country  47985 non-null  object \n",
      " 14  income          48842 non-null  object \n",
      "dtypes: float64(5), object(10)\n",
      "memory usage: 6.0+ MB\n",
      "None\n",
      "age                  0\n",
      "workclass         2800\n",
      "fnlwgt               1\n",
      "education            1\n",
      "education-num        1\n",
      "marital-status       1\n",
      "occupation        2810\n",
      "relationship         1\n",
      "race                 1\n",
      "sex                  1\n",
      "capital-gain         1\n",
      "capital-loss         1\n",
      "hours-per-week       1\n",
      "native-country     858\n",
      "income               1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check missing data and data type of columns\n",
    "print(dataset.info())\n",
    "print(dataset.isnull().sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: 147, datatype: object\n",
      "workclass: 8, datatype: object\n",
      "fnlwgt: 28523, datatype: float64\n",
      "education: 16, datatype: object\n",
      "education-num: 16, datatype: float64\n",
      "marital-status: 7, datatype: object\n",
      "occupation: 14, datatype: object\n",
      "relationship: 6, datatype: object\n",
      "race: 5, datatype: object\n",
      "sex: 2, datatype: object\n",
      "capital-gain: 123, datatype: float64\n",
      "capital-loss: 99, datatype: float64\n",
      "hours-per-week: 96, datatype: float64\n",
      "income: 2, datatype: object\n"
     ]
    }
   ],
   "source": [
    "# unique value count of each column\n",
    "for column in dataset.columns:\n",
    "    print(f\"{column}: {dataset[column].nunique()}, datatype: {dataset[column].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_type != 2:    \n",
    "    # extract X dataframe keeping column labels intact\n",
    "    X = dataset.iloc[:, :-1]\n",
    "    # extract y dataframe keeping column labels intact\n",
    "    Y = dataset.iloc[:, -1]\n",
    "\n",
    "    X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "0    284315\n",
      "1       492\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check class distribution of Y\n",
    "print(Y.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_train_split(X, Y):\n",
    "    # test train split\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n",
    "                                                        test_size = 0.2, random_state = 0)\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_labelEncode_oneHotEncode(X_df, columns_to_hot_encode, columns_to_label_encode):\n",
    "    #one hot encode\n",
    "    one_hot_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "    \n",
    "    if columns_to_hot_encode is not None:\n",
    "        X_df_encoded = one_hot_encoder.fit_transform(X_df[columns_to_hot_encode])\n",
    "        X_df = X_df.drop(columns_to_hot_encode, axis=1)\n",
    "        \n",
    "        #print(f\"shape of X_df before concat: {X_df.shape}\")\n",
    "        \n",
    "        X_df = pd.concat([X_df, pd.DataFrame(X_df_encoded)], axis=1)\n",
    "        \n",
    "        #print(f\"shape of X_df after concat: {X_df.shape}\")\n",
    "    \n",
    "    \n",
    "        \n",
    "    #label encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    if columns_to_label_encode is not None:\n",
    "        for column in columns_to_label_encode:\n",
    "            X_df[column] = label_encoder.fit_transform(X_df[column])\n",
    "        \n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_preprocess(X,Y, \n",
    "                      columns_to_hot_encode=None, \n",
    "                      columns_to_label_encode=None,\n",
    "                      columns_to_normalize=None,\n",
    "                      fill_missing_type='mean'):\n",
    "    \n",
    "    # one hot encode and label encode before splitting\n",
    "    X = global_labelEncode_oneHotEncode(X, columns_to_hot_encode, columns_to_label_encode)\n",
    "    X_df_train, X_df_test, Y_train, Y_test = get_test_train_split(X, Y)\n",
    "    \n",
    "    \n",
    "    # #convert to numeric and fill missing values\n",
    "    X_df_train = X_df_train.apply(pd.to_numeric, errors='coerce')\n",
    "    X_df_test = X_df_test.apply(pd.to_numeric, errors='coerce')\n",
    "    if fill_missing_type == 'zero':\n",
    "        X_df_train = X_df_train.fillna(0)\n",
    "        X_df_test = X_df_test.fillna(0)\n",
    "    else:    \n",
    "        X_df_train = X_df_train.fillna(X_df_train.mean())\n",
    "        X_df_test = X_df_test.fillna(X_df_test.mean())\n",
    "            \n",
    "    # #normalize\n",
    "    scaler = MinMaxScaler()\n",
    "    X_df_train[columns_to_normalize] = scaler.fit_transform(X_df_train[columns_to_normalize])\n",
    "    X_df_test[columns_to_normalize] = scaler.transform(X_df_test[columns_to_normalize])\n",
    "    \n",
    "    return X_df_train, X_df_test, Y_train, Y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the columns of X where data type is object and unique value count is 2\n",
    "# return list of those column names\n",
    "def get_binary_columns(X_binary):\n",
    "    binary_columns = []\n",
    "    for column in X_binary.columns:\n",
    "        if X_binary[column].dtype == 'object' and X_binary[column].nunique() == 2:\n",
    "            binary_columns.append(column)\n",
    "    return binary_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset specific functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset1(X, Y):\n",
    "    \n",
    "    #label encode Y\n",
    "    labelencoder_Y = LabelEncoder()\n",
    "    Y = labelencoder_Y.fit_transform(Y)\n",
    "    \n",
    "    X_df = X.drop('customerID', axis=1)\n",
    "    \n",
    "    X_df['MultipleLines'] = X_df['MultipleLines'].replace('No phone service', 'No')\n",
    "    internet_columns = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "    \n",
    "    for i in internet_columns:\n",
    "        X_df[i] = X_df[i].replace('No internet service', 'No')\n",
    "        \n",
    "    columns_to_hot_encode = ['InternetService', 'Contract', 'PaymentMethod']\n",
    "    # columns_to_label_encode = ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'OnlineSecurity', \n",
    "    #                            'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', \n",
    "    #                            'PaperlessBilling']\n",
    "    cols = get_binary_columns(X_df)    \n",
    "    columns_to_normalize = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_df_train, X_df_test, Y_train, Y_test = global_preprocess(X_df, Y, \n",
    "                                                               columns_to_hot_encode=columns_to_hot_encode,\n",
    "                                                                columns_to_label_encode=cols,\n",
    "                                                                columns_to_normalize=columns_to_normalize,\n",
    "                                                                fill_missing_type='mean')\n",
    "\n",
    "    return X_df_train, X_df_test, Y_train, Y_test\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset2(dataset_train, dataset_test):\n",
    "\n",
    "    X_train = dataset_train.iloc[:, :-1]\n",
    "    Y_train = dataset_train.iloc[:, -1]\n",
    "    X_test = dataset_test.iloc[:, :-1]\n",
    "    Y_test = dataset_test.iloc[:, -1]\n",
    "\n",
    "    \n",
    "    labelencoder_Y = LabelEncoder()\n",
    "    Y_train = labelencoder_Y.fit_transform(Y_train)\n",
    "    #Y_train = pd.DataFrame(Y_train)\n",
    "    Y_test = dataset_test.iloc[:, -1]\n",
    "    Y_test = labelencoder_Y.fit_transform(Y_test)\n",
    "    #Y_test = pd.DataFrame(Y_test)\n",
    "    \n",
    "    # replace ? values with nan\n",
    "    targert_columns = ['workclass', 'occupation', 'native-country']   \n",
    "    X_train[targert_columns] = X_train[targert_columns].replace(' ?', np.nan)\n",
    "    X_test[targert_columns] = X_test[targert_columns].replace(' ?', np.nan)\n",
    "    \n",
    "    \n",
    "    # list columns that are of type object\n",
    "    object_columns = X_train.select_dtypes(include=['object']).columns\n",
    "    columns_to_hot_encode = object_columns\n",
    "    X_train = global_labelEncode_oneHotEncode(X_train, columns_to_hot_encode, None)\n",
    "    X_test = global_labelEncode_oneHotEncode(X_test, columns_to_hot_encode, None)\n",
    "    #print(f\"columns_to_hot_encode: {columns_to_hot_encode}\")\n",
    "    \n",
    "    X_train.columns = X_train.columns.astype(str)\n",
    "    X_test.columns = X_test.columns.astype(str)\n",
    "    # list columns that are of type numeric for nomralization\n",
    "    numeric_columns = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    columns_to_normalize = numeric_columns\n",
    "    #print(f\"columns_to_normalize: {columns_to_normalize}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # #convert to numeric and fill missing values\n",
    "    X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "    X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    X_train = X_train.fillna(X_train.mean())\n",
    "    X_test = X_test.fillna(X_test.mean())\n",
    "            \n",
    "    # #normalize\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train[columns_to_normalize] = scaler.fit_transform(X_train[columns_to_normalize])\n",
    "    X_test[columns_to_normalize] = scaler.transform(X_test[columns_to_normalize])\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset3(X, Y):\n",
    "    \n",
    "    # list columns that are of type numeric for nomralization\n",
    "    numeric_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    columns_to_normalize = numeric_columns\n",
    "    \n",
    "    #randomly select 2000 '0' class samples from Y and all '1' class samples\n",
    "    concat_df = pd.concat([X, Y], axis=1)\n",
    "    concat_df_0 = concat_df[concat_df['Class'] == 0]\n",
    "    concat_df_1 = concat_df[concat_df['Class'] == 1]\n",
    "    concat_df_0_sample = concat_df_0.sample(n=2000, random_state=0)\n",
    "    concat_df_1_sample = concat_df_1\n",
    "    concat_df_sample = pd.concat([concat_df_0_sample, concat_df_1_sample], axis=0)\n",
    "    X = concat_df_sample.drop('Class', axis=1)\n",
    "    Y = concat_df_sample['Class']\n",
    "    \n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = global_preprocess(X, Y, \n",
    "                                                        columns_to_hot_encode=None,\n",
    "                                                        columns_to_label_encode=None,\n",
    "                                                        columns_to_normalize=columns_to_normalize,\n",
    "                                                        fill_missing_type='mean')\n",
    "\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_type == 1:\n",
    "    X_train, X_test, Y_train, Y_test = preprocess_dataset1(X, Y)\n",
    "elif dataset_type == 2:\n",
    "    X_train, X_test, Y_train, Y_test = preprocess_dataset2(dataset2_train, dataset2_test)\n",
    "elif dataset_type == 3:\n",
    "    X_train, X_test, Y_train, Y_test = preprocess_dataset3(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42549</th>\n",
       "      <td>0.237758</td>\n",
       "      <td>0.766287</td>\n",
       "      <td>0.474491</td>\n",
       "      <td>0.641708</td>\n",
       "      <td>0.775829</td>\n",
       "      <td>0.493591</td>\n",
       "      <td>0.242183</td>\n",
       "      <td>0.544067</td>\n",
       "      <td>0.728048</td>\n",
       "      <td>0.340158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497793</td>\n",
       "      <td>0.502577</td>\n",
       "      <td>0.565251</td>\n",
       "      <td>0.582999</td>\n",
       "      <td>0.522098</td>\n",
       "      <td>0.432747</td>\n",
       "      <td>0.388567</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.577907</td>\n",
       "      <td>0.020566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239781</th>\n",
       "      <td>0.870461</td>\n",
       "      <td>0.912390</td>\n",
       "      <td>0.373862</td>\n",
       "      <td>0.921117</td>\n",
       "      <td>0.252656</td>\n",
       "      <td>0.669774</td>\n",
       "      <td>0.476085</td>\n",
       "      <td>0.731555</td>\n",
       "      <td>0.674641</td>\n",
       "      <td>0.658300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444579</td>\n",
       "      <td>0.460811</td>\n",
       "      <td>0.570887</td>\n",
       "      <td>0.597741</td>\n",
       "      <td>0.543073</td>\n",
       "      <td>0.510574</td>\n",
       "      <td>0.258370</td>\n",
       "      <td>0.704874</td>\n",
       "      <td>0.543645</td>\n",
       "      <td>0.002681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215915</th>\n",
       "      <td>0.812524</td>\n",
       "      <td>0.914583</td>\n",
       "      <td>0.400650</td>\n",
       "      <td>0.838133</td>\n",
       "      <td>0.263102</td>\n",
       "      <td>0.682637</td>\n",
       "      <td>0.438572</td>\n",
       "      <td>0.718797</td>\n",
       "      <td>0.633072</td>\n",
       "      <td>0.612232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.459246</td>\n",
       "      <td>0.436203</td>\n",
       "      <td>0.583058</td>\n",
       "      <td>0.604928</td>\n",
       "      <td>0.232527</td>\n",
       "      <td>0.495088</td>\n",
       "      <td>0.279943</td>\n",
       "      <td>0.672657</td>\n",
       "      <td>0.517899</td>\n",
       "      <td>0.003494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41272</th>\n",
       "      <td>0.234697</td>\n",
       "      <td>0.874487</td>\n",
       "      <td>0.416546</td>\n",
       "      <td>0.876844</td>\n",
       "      <td>0.351772</td>\n",
       "      <td>0.658422</td>\n",
       "      <td>0.436529</td>\n",
       "      <td>0.720179</td>\n",
       "      <td>0.694391</td>\n",
       "      <td>0.560626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425019</td>\n",
       "      <td>0.461192</td>\n",
       "      <td>0.532310</td>\n",
       "      <td>0.599975</td>\n",
       "      <td>0.408824</td>\n",
       "      <td>0.548590</td>\n",
       "      <td>0.252547</td>\n",
       "      <td>0.631218</td>\n",
       "      <td>0.499015</td>\n",
       "      <td>0.000233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7945</th>\n",
       "      <td>0.062790</td>\n",
       "      <td>0.962840</td>\n",
       "      <td>0.349690</td>\n",
       "      <td>0.909012</td>\n",
       "      <td>0.293089</td>\n",
       "      <td>0.642469</td>\n",
       "      <td>0.425801</td>\n",
       "      <td>0.719380</td>\n",
       "      <td>0.671096</td>\n",
       "      <td>0.696351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439050</td>\n",
       "      <td>0.455976</td>\n",
       "      <td>0.525991</td>\n",
       "      <td>0.604390</td>\n",
       "      <td>0.494035</td>\n",
       "      <td>0.564015</td>\n",
       "      <td>0.561738</td>\n",
       "      <td>0.667063</td>\n",
       "      <td>0.509673</td>\n",
       "      <td>0.009091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "42549   0.237758  0.766287  0.474491  0.641708  0.775829  0.493591  0.242183   \n",
       "239781  0.870461  0.912390  0.373862  0.921117  0.252656  0.669774  0.476085   \n",
       "215915  0.812524  0.914583  0.400650  0.838133  0.263102  0.682637  0.438572   \n",
       "41272   0.234697  0.874487  0.416546  0.876844  0.351772  0.658422  0.436529   \n",
       "7945    0.062790  0.962840  0.349690  0.909012  0.293089  0.642469  0.425801   \n",
       "\n",
       "              V7        V8        V9  ...       V20       V21       V22  \\\n",
       "42549   0.544067  0.728048  0.340158  ...  0.497793  0.502577  0.565251   \n",
       "239781  0.731555  0.674641  0.658300  ...  0.444579  0.460811  0.570887   \n",
       "215915  0.718797  0.633072  0.612232  ...  0.459246  0.436203  0.583058   \n",
       "41272   0.720179  0.694391  0.560626  ...  0.425019  0.461192  0.532310   \n",
       "7945    0.719380  0.671096  0.696351  ...  0.439050  0.455976  0.525991   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28    Amount  \n",
       "42549   0.582999  0.522098  0.432747  0.388567  0.884977  0.577907  0.020566  \n",
       "239781  0.597741  0.543073  0.510574  0.258370  0.704874  0.543645  0.002681  \n",
       "215915  0.604928  0.232527  0.495088  0.279943  0.672657  0.517899  0.003494  \n",
       "41272   0.599975  0.408824  0.548590  0.252547  0.631218  0.499015  0.000233  \n",
       "7945    0.604390  0.494035  0.564015  0.561738  0.667063  0.509673  0.009091  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16282 entries, 0 to 16281\n",
      "Columns: 104 entries, age to 97\n",
      "dtypes: float64(104)\n",
      "memory usage: 12.9 MB\n",
      "None\n",
      "age              0\n",
      "fnlwgt           0\n",
      "education-num    0\n",
      "capital-gain     0\n",
      "capital-loss     0\n",
      "                ..\n",
      "93               0\n",
      "94               0\n",
      "95               0\n",
      "96               0\n",
      "97               0\n",
      "Length: 104, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check missing data and data type of columns\n",
    "print(X_test.info())\n",
    "print(X_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(32561, 1)\n"
     ]
    }
   ],
   "source": [
    "#print type and shape of X_train\n",
    "print(type(Y_train))\n",
    "print(Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(labels):\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = counts / len(labels)\n",
    "    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_by_IG(X_train, X_test, Y_train, num_features=-1):\n",
    "    base_entropy = entropy(Y_train)\n",
    "    print(f\"base entropy: {base_entropy}\")\n",
    "    \n",
    "    preprocessed_X = X_train.copy()\n",
    "    \n",
    "    # calculate entropy for each feature\n",
    "    entropies = []\n",
    "    InfoGain = []\n",
    "    for column in preprocessed_X.columns:\n",
    "        feat_value, feat_value_counts = np.unique(preprocessed_X[column], return_counts=True)\n",
    "        weighted_feature_entropy = 0\n",
    "        \n",
    "        for value, count in zip(feat_value, feat_value_counts):\n",
    "            #weighted_feature_entropy += count * entropy(Y_train[preprocessed_X[column] == value])\n",
    "            weighted_feature_entropy += (count / len(preprocessed_X)) * entropy(Y_train[preprocessed_X[column] == value])\n",
    "            \n",
    "            \n",
    "        entropies.append(weighted_feature_entropy)\n",
    "        InfoGain.append(base_entropy - weighted_feature_entropy)\n",
    "        #print(f\"entropy for column '{column}': {weighted_feature_entropy} and information gain: {base_entropy - weighted_feature_entropy}\" )\n",
    "        \n",
    "    # sort by InfoGain\n",
    "    sorted_indices = np.argsort(InfoGain)[::-1]\n",
    "    sorted_IG = np.sort(InfoGain)[::-1]\n",
    "    sorted_columns = preprocessed_X.columns[sorted_indices]\n",
    "    \n",
    "    #print(sorted_IG)\n",
    "    \n",
    "    if num_features == -1:\n",
    "        num_features = len(sorted_columns)\n",
    "        \n",
    "    # return dataframe with selected features and extract the same features from the test set\n",
    "    truncated_X_train = pd.DataFrame(preprocessed_X[sorted_columns[:num_features]], columns=sorted_columns[:num_features])\n",
    "    \n",
    "    # only select those features from test set which are present in the truncated_X_train\n",
    "    truncated_X_test = pd.DataFrame(X_test[sorted_columns[:num_features]], columns=sorted_columns[:num_features])\n",
    "    \n",
    "    return truncated_X_train, truncated_X_test\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base entropy: 0.7091021926507215\n",
      "shape of X_train: (1993, 30)\n",
      "shape of X_test: (499, 30)\n",
      "shape of Y_train: (1993,) and type: <class 'pandas.core.series.Series'>\n",
      "shape of Y_test: (499,) and type: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "trunc_X_train, trunc_X_test = feature_selection_by_IG(X_train, X_test, Y_train, num_features=-1)\n",
    "\n",
    "# make Y_train and Y_test as numpy arrays\n",
    "# Y_train = Y_train.values\n",
    "# Y_test = Y_test.values\n",
    "\n",
    "print(f\"shape of X_train: {trunc_X_train.shape}\")\n",
    "print(f\"shape of X_test: {trunc_X_test.shape}\")\n",
    "print(f\"shape of Y_train: {Y_train.shape} and type: {type(Y_train)}\")\n",
    "print(f\"shape of Y_test: {Y_test.shape} and type: {type(Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified Logistic regression that also works as a weak learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modified_regressor:\n",
    "    def __init__(self, X_train, Y_train, \n",
    "                threshold=-1, max_feature_count=-1, learning_rate=0.1):\n",
    "        self.X = X_train\n",
    "        self.Y = Y_train\n",
    "        self.X_test = None\n",
    "        self.Y_test = None\n",
    "        \n",
    "        self.y_hat = None\n",
    "        self.weights = None\n",
    "        self.max_feature_count = max_feature_count\n",
    "        self.threshold = threshold\n",
    "        self.learning_rate = learning_rate\n",
    "        self.selected_features = None\n",
    "        \n",
    "        self.sorted_indices = None\n",
    "        self.sorted_IG = None\n",
    "        self.sorted_columns = None\n",
    "            \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def binary_cross_entropy(self, y, y_hat):\n",
    "        return -(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    \n",
    "    def gradient_descent(self):\n",
    "        z = np.dot(self.X, self.weights)\n",
    "        y_hat = self.sigmoid(z)\n",
    "        error = y_hat - self.Y\n",
    "        gradient = np.dot(self.X.T, error) / len(self.Y)\n",
    "        self.weights -= self.learning_rate * gradient\n",
    "    \n",
    "    def train(self):        \n",
    "        self.weights = np.zeros(self.X.shape[1])\n",
    "        iteration = 0\n",
    "        if self.threshold is not None and self.threshold > 0:\n",
    "            error = float('inf')\n",
    "            \n",
    "            \n",
    "            steps = 1000\n",
    "            for i in range(steps):\n",
    "                self.gradient_descent()\n",
    "                iteration += 1\n",
    "                z = np.dot(self.X, self.weights)\n",
    "                y_hat = self.sigmoid(z)\n",
    "                sum_error = np.sum(self.binary_cross_entropy(self.Y, y_hat))\n",
    "                error = sum_error / len(self.Y)\n",
    "                if error <= self.threshold:\n",
    "                    break\n",
    "        else:\n",
    "            steps = 1000\n",
    "            for i in range(steps):\n",
    "                iteration += 1\n",
    "                self.gradient_descent()\n",
    "                z = np.dot(self.X, self.weights)\n",
    "                y_hat = self.sigmoid(z)\n",
    "                sum_error = np.sum(self.binary_cross_entropy(self.Y, y_hat))\n",
    "                error = sum_error / len(self.Y)\n",
    "        \n",
    "        #print(f\"total iterations: {iteration}, error: {error}\")        \n",
    "        #return self.weights, error, iteration\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        self.X_test = X_test\n",
    "        z = np.dot(self.X_test, self.weights)\n",
    "        y_hat = np.round(self.sigmoid(z))\n",
    "        self.y_hat = y_hat\n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "    def accuracy(self, Y_test):\n",
    "        self.Y_test = Y_test\n",
    "        return (np.sum(self.y_hat == self.Y_test) / len(self.Y_test))*100\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost(examples_X, examples_Y, L_weak, K, num_features=-1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    examples : set of N examples\n",
    "    L_weak : weak learner (logistic regression)\n",
    "    K : number of weak learners to use\n",
    "    \n",
    "    ------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    data_point_weights = np.ones(len(examples_X)) / len(examples_X)\n",
    "    hypothesises = []\n",
    "    hypo_weights = []\n",
    "        \n",
    "    for i in range(K):\n",
    "        #resampling data\n",
    "        # resampling data\n",
    "        indices = np.random.choice(len(examples_X), len(examples_X), p=data_point_weights)\n",
    "        resampled_examples_X = examples_X.values[indices]\n",
    "        resampled_examples_Y = examples_Y[indices]\n",
    "\n",
    "        \n",
    "        # initialize regressor\n",
    "        regressor = L_weak(resampled_examples_X, resampled_examples_Y, 0.5, num_features)\n",
    "        regressor.train()\n",
    "        y_hat = regressor.predict(examples_X)\n",
    "        #hypothesises.append(regressor)\n",
    "        \n",
    "        error = 0\n",
    "        for j in range(len(examples_X)):\n",
    "            if y_hat[j] != examples_Y[j]:\n",
    "                error += data_point_weights[j]\n",
    "            \n",
    "        if error > 0.5:\n",
    "            continue\n",
    "        else:\n",
    "            hypothesises.append(regressor)\n",
    "        \n",
    "        for j in range(len(examples_X)):\n",
    "            if y_hat[j] == examples_Y[j]:\n",
    "                data_point_weights[j] *= error / (1 - error)\n",
    "        \n",
    "        # normalizing weights\n",
    "        data_point_weights /= np.sum(data_point_weights)\n",
    "        \n",
    "        hypo_weights.append(np.log2((1 - error) / (error + 1e-10)))\n",
    "        \n",
    "    return hypothesises, hypo_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_majority(hypothesises, hypo_weights, examples_X):\n",
    "    predictions = []\n",
    "    \n",
    "    #normalize hypothesis weights\n",
    "    hypo_weights /= np.sum(hypo_weights)\n",
    "    \n",
    "    y_hats = np.zeros(len(examples_X))\n",
    "    \n",
    "    for i in range(len(hypothesises)):\n",
    "        y_hats += hypo_weights[i] * hypothesises[i].predict(examples_X)\n",
    "    \n",
    "    predictions = np.round(y_hats)\n",
    "    \n",
    "    \n",
    "    # for i in range(len(examples_X)):\n",
    "    #     prediction = 0\n",
    "    #     for j in range(len(hypothesises)):\n",
    "    #         prediction += hypo_weights[j] * hypothesises[j].predict(examples_X[i])  \n",
    "    #     prediction /= len(hypothesises)\n",
    "    #     predictions.append(np.round(prediction))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_metrics(pred_Y, true_Y):\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    \n",
    "    for i in range(len(pred_Y)):\n",
    "        if pred_Y[i] == true_Y[i] == 1:\n",
    "            TP += 1\n",
    "        elif pred_Y[i] == 1 and true_Y[i] == 0:\n",
    "            FP += 1\n",
    "        elif pred_Y[i] == 0 and true_Y[i] == 1:\n",
    "            FN += 1\n",
    "        else:\n",
    "            TN += 1\n",
    "            \n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    false_discovery_rate = FP / (FP + TP)\n",
    "    F1_score = 2 / ((1 / precision) + (1 / sensitivity))\n",
    "    \n",
    "    return accuracy, precision, sensitivity, specificity, false_discovery_rate, F1_score\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stuffs(accuracy, sensitivity, specificity, precision, false_discovery_rate, F1_score):\n",
    "    print(f\"accuracy: {accuracy}\")\n",
    "    print(f\"precision: {precision}\")\n",
    "    print(f\"sensitivity: {sensitivity}\")\n",
    "    print(f\"specificity: {specificity}\")\n",
    "    print(f\"false_discovery_rate: {false_discovery_rate}\")\n",
    "    print(f\"F1_score: {F1_score}\")\n",
    "    print(\"\\n---------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Without Boosting:\n",
      "\n",
      "Training data\n",
      "accuracy: 0.8264181075519793\n",
      "precision: 0.6994714780390012\n",
      "sensitivity: 0.48947838285932915\n",
      "specificity: 0.9332928802588997\n",
      "false_discovery_rate: 0.3005285219609987\n",
      "F1_score: 0.5759303721488594\n",
      "\n",
      "---------------\n",
      "\n",
      "Test data\n",
      "accuracy: 0.7755803955288049\n",
      "precision: 0.628\n",
      "sensitivity: 0.12246489859594384\n",
      "specificity: 0.9775651334834352\n",
      "false_discovery_rate: 0.372\n",
      "F1_score: 0.20496083550913838\n",
      "\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if Y_train is series.series conver to ndarray\n",
    "if type(Y_train) == pd.core.series.Series:\n",
    "    Y_train = Y_train.values\n",
    "if type(Y_test) == pd.core.series.Series:\n",
    "    Y_test = Y_test.values\n",
    "\n",
    "regressor = modified_regressor(trunc_X_train, Y_train)\n",
    "regressor.train()\n",
    "\n",
    "print(f\"\\nWithout Boosting:\\n\")\n",
    "print(f\"Training data\")\n",
    "\n",
    "y_pred = regressor.predict(trunc_X_train)\n",
    "\n",
    "\n",
    "accuracy, precision, sensitivity, specificity, false_discovery_rate, F1_score = perf_metrics(y_pred, Y_train)\n",
    "print_stuffs(accuracy, sensitivity, specificity, precision, false_discovery_rate, F1_score)\n",
    "\n",
    "print(f\"Test data\")\n",
    "\n",
    "y_pred = regressor.predict(trunc_X_test)\n",
    "accuracy, precision, sensitivity, specificity, false_discovery_rate, F1_score = perf_metrics(y_pred, Y_test)\n",
    "print_stuffs(accuracy, sensitivity, specificity, precision, false_discovery_rate, F1_score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesises, hypo_weights = adaboost(trunc_X_train, trunc_X_test, Y_train, Y_test, modified_regressor, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shape of truncated_X_train: {trunc_X_train.shape}\")\n",
    "print(f\"shape of truncated_X_test: {trunc_X_test.shape}\")\n",
    "print(f\"type of truncated_X_train: {type(trunc_X_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predictions\n",
    "print(f\"Training data\")\n",
    "y_hat = weighted_majority(hypothesises, hypo_weights, trunc_X_train)\n",
    "accuracy, precision, sensitivity, specificity, false_discovery_rate, F1_score = perf_metrics(y_hat, Y_train)\n",
    "print_stuffs(accuracy, sensitivity, specificity, precision, false_discovery_rate, F1_score)\n",
    "\n",
    "print(f\"Test data\")\n",
    "y_hat = weighted_majority(hypothesises, hypo_weights, trunc_X_test)\n",
    "accuracy, precision, sensitivity, specificity, false_discovery_rate, F1_score = perf_metrics(y_hat, Y_test)\n",
    "print_stuffs(accuracy, sensitivity, specificity, precision, false_discovery_rate, F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 5\n",
      "Training data\n",
      "accuracy: 0.9518314099347717\n",
      "precision: 1.0\n",
      "sensitivity: 0.7512953367875648\n",
      "specificity: 1.0\n",
      "false_discovery_rate: 0.0\n",
      "F1_score: 0.8579881656804734\n",
      "\n",
      "---------------\n",
      "\n",
      "Test data\n",
      "accuracy: 0.9539078156312625\n",
      "precision: 1.0\n",
      "sensitivity: 0.7830188679245284\n",
      "specificity: 1.0\n",
      "false_discovery_rate: 0.0\n",
      "F1_score: 0.8783068783068784\n",
      "\n",
      "---------------\n",
      "\n",
      "K: 10\n",
      "Training data\n",
      "accuracy: 0.963371801304566\n",
      "precision: 0.924119241192412\n",
      "sensitivity: 0.883419689119171\n",
      "specificity: 0.9825762289981331\n",
      "false_discovery_rate: 0.07588075880758807\n",
      "F1_score: 0.9033112582781457\n",
      "\n",
      "---------------\n",
      "\n",
      "Test data\n",
      "accuracy: 0.9478957915831663\n",
      "precision: 0.8773584905660378\n",
      "sensitivity: 0.8773584905660378\n",
      "specificity: 0.9669211195928753\n",
      "false_discovery_rate: 0.12264150943396226\n",
      "F1_score: 0.8773584905660379\n",
      "\n",
      "---------------\n",
      "\n",
      "K: 15\n",
      "Training data\n",
      "accuracy: 0.9678876066231812\n",
      "precision: 0.9497206703910615\n",
      "sensitivity: 0.8808290155440415\n",
      "specificity: 0.9887990043559427\n",
      "false_discovery_rate: 0.05027932960893855\n",
      "F1_score: 0.9139784946236558\n",
      "\n",
      "---------------\n",
      "\n",
      "Test data\n",
      "accuracy: 0.9579158316633266\n",
      "precision: 0.9207920792079208\n",
      "sensitivity: 0.8773584905660378\n",
      "specificity: 0.9796437659033079\n",
      "false_discovery_rate: 0.07920792079207921\n",
      "F1_score: 0.8985507246376813\n",
      "\n",
      "---------------\n",
      "\n",
      "K: 20\n",
      "Training data\n",
      "accuracy: 0.9668840943301555\n",
      "precision: 0.9444444444444444\n",
      "sensitivity: 0.8808290155440415\n",
      "specificity: 0.9875544492843809\n",
      "false_discovery_rate: 0.05555555555555555\n",
      "F1_score: 0.9115281501340482\n",
      "\n",
      "---------------\n",
      "\n",
      "Test data\n",
      "accuracy: 0.9559118236472945\n",
      "precision: 0.9117647058823529\n",
      "sensitivity: 0.8773584905660378\n",
      "specificity: 0.9770992366412213\n",
      "false_discovery_rate: 0.08823529411764706\n",
      "F1_score: 0.8942307692307693\n",
      "\n",
      "---------------\n",
      "\n",
      "K: 25\n",
      "Training data\n",
      "accuracy: 0.9628700451580532\n",
      "precision: 0.9171122994652406\n",
      "sensitivity: 0.8886010362694301\n",
      "specificity: 0.9807093963907902\n",
      "false_discovery_rate: 0.08288770053475936\n",
      "F1_score: 0.9026315789473683\n",
      "\n",
      "---------------\n",
      "\n",
      "Test data\n",
      "accuracy: 0.9478957915831663\n",
      "precision: 0.8703703703703703\n",
      "sensitivity: 0.8867924528301887\n",
      "specificity: 0.9643765903307888\n",
      "false_discovery_rate: 0.12962962962962962\n",
      "F1_score: 0.8785046728971962\n",
      "\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for K in range(5, 30, 5):\n",
    "    print(f\"K: {K}\")\n",
    "    hypothesises, hypo_weights = adaboost(trunc_X_train, Y_train,  \n",
    "                                          modified_regressor, K)\n",
    "    ## predictions\n",
    "    print(f\"Training data\")\n",
    "    y_hat = weighted_majority(hypothesises, hypo_weights, trunc_X_train)\n",
    "    accuracy, precision, sensitivity, specificity, false_discovery_rate, F1_score = perf_metrics(y_hat, Y_train)\n",
    "    print_stuffs(accuracy, sensitivity, specificity, precision, false_discovery_rate, F1_score)\n",
    "\n",
    "    print(f\"Test data\")\n",
    "    y_hat = weighted_majority(hypothesises, hypo_weights, trunc_X_test)\n",
    "    accuracy, precision, sensitivity, specificity, false_discovery_rate, F1_score = perf_metrics(y_hat, Y_test)\n",
    "    print_stuffs(accuracy, sensitivity, specificity, precision, false_discovery_rate, F1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "accuracy: 0.8363789926289926\n",
      "precision: 0.6803636363636364\n",
      "sensitivity: 0.5991034261927634\n",
      "specificity: 0.9112210887789112\n",
      "false_discovery_rate: 0.31963636363636366\n",
      "F1_score: 0.6371530733866848\n",
      "\n",
      "---------------\n",
      "\n",
      "Test data\n",
      "accuracy: 0.8309534776600644\n",
      "precision: 0.6708160442600276\n",
      "sensitivity: 0.6081504702194357\n",
      "specificity: 0.9032126880845872\n",
      "false_discovery_rate: 0.32918395573997233\n",
      "F1_score: 0.637948043406774\n",
      "\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hypothesises, hypo_weights = adaboost(trunc_X_train, Y_train,  \n",
    "                                        modified_regressor, 25)\n",
    "## predictions\n",
    "print(f\"Training data\")\n",
    "y_hat = weighted_majority(hypothesises, hypo_weights, trunc_X_train)\n",
    "accuracy, precision, sensitivity, specificity, false_discovery_rate, F1_score = perf_metrics(y_hat, Y_train)\n",
    "print_stuffs(accuracy, sensitivity, specificity, precision, false_discovery_rate, F1_score)\n",
    "\n",
    "print(f\"Test data\")\n",
    "y_hat = weighted_majority(hypothesises, hypo_weights, trunc_X_test)\n",
    "accuracy, precision, sensitivity, specificity, false_discovery_rate, F1_score = perf_metrics(y_hat, Y_test)\n",
    "print_stuffs(accuracy, sensitivity, specificity, precision, false_discovery_rate, F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
